{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HExLQrE4ZxR"
   },
   "source": [
    "<h1><font color='blue'> 8E and 8F: Finding the Probability P(Y==1|X)</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LuKrFzC4ZxV"
   },
   "source": [
    "<h2><font color='Geen'> 8E: Implementing Decision Function of SVM RBF Kernel</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wES-wWN4ZxX"
   },
   "source": [
    "<font face=' Comic Sans MS' size=3>After we train a kernel SVM model, we will be getting support vectors and their corresponsing coefficients $\\alpha_{i}$\n",
    "\n",
    "Check the documentation for better understanding of these attributes: \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "<img src='https://i.imgur.com/K11msU4.png' width=500>\n",
    "\n",
    "As a part of this assignment you will be implementing the ```decision_function()``` of kernel SVM, here decision_function() means based on the value return by ```decision_function()``` model will classify the data point either as positive or negative\n",
    "\n",
    "Ex 1: In logistic regression After traning the models with the optimal weights $w$ we get, we will find the value $\\frac{1}{1+\\exp(-(wx+b))}$, if this value comes out to be < 0.5 we will mark it as negative class, else its positive class\n",
    "\n",
    "Ex 2: In Linear SVM After traning the models with the optimal weights $w$ we get, we will find the value of $sign(wx+b)$, if this value comes out to be -ve we will mark it as negative class, else its positive class.\n",
    "\n",
    "Similarly in Kernel SVM After traning the models with the coefficients $\\alpha_{i}$ we get, we will find the value of \n",
    "$sign(\\sum_{i=1}^{n}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here $K(x_{i},x_{q})$ is the RBF kernel. If this value comes out to be -ve we will mark $x_{q}$ as negative class, else its positive class.\n",
    "\n",
    "RBF kernel is defined as: $K(x_{i},x_{q})$ = $exp(-\\gamma ||x_{i} - x_{q}||^2)$\n",
    "\n",
    "For better understanding check this link: https://scikit-learn.org/stable/modules/svm.html#svm-mathematical-formulation\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z830CfMk4Zxa"
   },
   "source": [
    "## Task E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuBxHiCQ4Zxc"
   },
   "source": [
    "> 1. Split the data into $X_{train}$(60), $X_{cv}$(20), $X_{test}$(20)\n",
    "\n",
    "> 2. Train $SVC(gamma=0.001, C=100.)$ on the ($X_{train}$, $y_{train}$)\n",
    "\n",
    "> 3. Get the decision boundry values $f_{cv}$ on the $X_{cv}$ data  i.e. ` `$f_{cv}$ ```= decision_function(```$X_{cv}$```)```  <font color='red'>you need to implement this decision_function()</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "id": "fCgMNEvI4Zxf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "id": "ANUNIqCe4Zxn"
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=5000, n_features=5, n_redundant=2,\n",
    "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHie1zqH4Zxt"
   },
   "source": [
    "### Pseudo code\n",
    "\n",
    "clf = SVC(gamma=0.001, C=100.)<br>\n",
    "clf.fit(Xtrain, ytrain)\n",
    "\n",
    "<font color='green'>def</font> <font color='blue'>decision_function</font>(Xcv, ...): #use appropriate parameters <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='green'>for</font> a data point $x_q$ <font color='green'>in</font> Xcv: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='grey'>#write code to implement $(\\sum_{i=1}^{\\text{all the support vectors}}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here the values $y_i$, $\\alpha_{i}$, and $intercept$ can be obtained from the trained model</font><br>\n",
    "   <font color='green'>return</font> <font color='grey'><i># the decision_function output for all the data points in the Xcv</i></font>\n",
    "    \n",
    "fcv = decision_function(Xcv, ...)  <i># based on your requirement you can pass any other parameters </i>\n",
    "\n",
    "<b>Note</b>: Make sure the values you get as fcv, should be equal to outputs of clf.decision_function(Xcv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "id": "h43kDT3M41u5"
   },
   "outputs": [],
   "source": [
    "clf = SVC(gamma = 0.001 , C=100)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "sv = clf.support_vectors_\n",
    "intercept = clf.intercept_\n",
    "coef = clf.dual_coef_\n",
    "\n",
    "def decision_function(data):\n",
    "    lst = []\n",
    "    for xq in data:  \n",
    "        sum = 0\n",
    "        for i in range(len(sv)):\n",
    "            sum += (  ( coef[0][i]  *  (np.exp( -0.001 * (np.linalg.norm(sv[i]- xq))**2 ) ) ) )\n",
    "        lst.append(sum+intercept) #intercept to added after summation\n",
    "    flst = np.array(lst)\n",
    "    return flst\n",
    "        \n",
    "fcv = decision_function(X_cv).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0bKCboN4Zxu"
   },
   "source": [
    "<h2><font color='Geen'> 8F: Implementing Platt Scaling to find P(Y==1|X)</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMn7OEN94Zxw"
   },
   "source": [
    "Check this <a href='https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a'>PDF</a>\n",
    "<img src='https://i.imgur.com/CAMnVnh.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0n5EFkx4Zxz"
   },
   "source": [
    "## TASK F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0HOqVJq4Zx1"
   },
   "source": [
    "\n",
    "> 4. Apply SGD algorithm with ($f_{cv}$, $y_{cv}$) and find the weight $W$ intercept $b$ ```Note: here our data is of one dimensional so we will have a one dimensional weight vector i.e W.shape (1,)``` \n",
    "\n",
    "> Note1: Don't forget to change the values of $y_{cv}$ as mentioned in the above image. you will calculate y+, y- based on data points in train data\n",
    "\n",
    "> Note2: the Sklearn's SGD algorithm doesn't support the real valued outputs, you need to use the code that was done in the `'Logistic Regression with SGD and L2'` Assignment after modifying loss function, and use same parameters that used in that assignment.\n",
    "<img src='https://i.imgur.com/zKYE9Oc.png'>\n",
    "if Y[i] is 1, it will be replaced with y+ value else it will replaced with y- value\n",
    "\n",
    "> 5. For a given data point from $X_{test}$, $P(Y=1|X) = \\frac{1}{1+exp(-(W*f_{test}+ b))}$ where ` `$f_{test}$ ```= decision_function(```$X_{test}$```)```, W and b will be learned as metioned in the above step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_p = 0\n",
    "N_n = 0\n",
    "y_p = 0\n",
    "y_n = 0\n",
    "\n",
    "for i in fcv:\n",
    "    if i<0:\n",
    "        N_n+=1\n",
    "    else:\n",
    "        N_p+=1\n",
    "        \n",
    "\n",
    "\n",
    "y_p = ((N_p+1)/(N_n+2))\n",
    "y_n = 1/(N_n+2)\n",
    "\n",
    "y_cv_new = np.where(y_cv==0, y_n, y_p)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:01<00:00, 25.70it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def initialize_weights(dim):\n",
    "    w = np.zeros_like(dim)\n",
    "    b = 0 \n",
    "    return w,b\n",
    "\n",
    "def sigmoid(z):\n",
    "    ''' In this function, we will return sigmoid of z'''\n",
    "    # compute sigmoid(z) and return\n",
    "    sig = 1 /(1+ (np.exp(-1*z)))\n",
    "    return sig\n",
    "\n",
    "def logloss(y_true,y_pred):\n",
    "    '''In this function, we will compute log loss '''\n",
    "    ln_arr = len(y_true)\n",
    "    loss = 0 \n",
    "    for i in range(ln_arr):\n",
    "        loss += (y_true[i] * np.log10(y_pred[i])) + ((1-y_true[i]) * np.log10(1-y_pred[i]))   \n",
    "    loss = (loss * -1)/ln_arr\n",
    "    return loss\n",
    "\n",
    "def gradient_dw(x,y,w,b,alpha,N):\n",
    "    '''In this function, we will compute the gardient w.r.to w '''\n",
    "#     dw = []\n",
    "#     for i in range(len(x)):\n",
    "    f = (x *( y - sigmoid (np.dot(w.T,x) + b   )  ))   -  ((alpha *w)/N)\n",
    "#     dw.append(f)   \n",
    "    dw = np.array(f)\n",
    "    return dw\n",
    "\n",
    "def gradient_db(x,y,w,b):\n",
    "    '''In this function, we will compute gradient w.r.to b '''\n",
    "    db = y - sigmoid(np.dot(w.T,x) + b)\n",
    "            \n",
    "    return db\n",
    "\n",
    "def pred(w,b, X):\n",
    "    N = len(X)\n",
    "    predict = []\n",
    "    for i in range(N):\n",
    "        z=np.dot(w,X[i])+b\n",
    "        if sigmoid(z) >= 0.5: # sigmoid(w,x,b) returns 1/(1+exp(-(dot(x,w)+b)))\n",
    "            predict.append(0.99999)  #to avoid division by zero error\n",
    "        else:\n",
    "            predict.append(0.00001)  #to avoid division by zero error\n",
    "    return np.array(predict)\n",
    "\n",
    "def train(X_train,y_train,epochs,alpha,eta0):\n",
    "    dim=X_train[0] \n",
    "    w,b = initialize_weights(dim)\n",
    "    train_loss = [] #list of trainloss\n",
    "    test_loss = []  #list of test loss\n",
    "    e = []          #epoch number\n",
    "    for epoch in tqdm(range(epochs)):   #for every epoch\n",
    "        for x , y  in zip(X_train , y_train):    #for every point \n",
    "            gw = gradient_dw(x,y,w,b,alpha,len(X_train))    \n",
    "            gb = gradient_db(x,y,w,b)\n",
    "            w = w + (eta0*gw)\n",
    "            b = b + (eta0*gb)\n",
    "        train_loss.append(logloss(y_train ,pred(w,b, X_train)))\n",
    "        e.append(epoch+1)\n",
    "    return w,b,train_loss,e\n",
    "\n",
    "alpha=0.0001\n",
    "eta0=0.0001\n",
    "epochs=50\n",
    "\n",
    "w,b,train_loss ,e=train(fcv, y_cv_new , epochs , alpha , eta0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P==1|X (for every pointin Xtest):  [[0.05232184]\n",
      " [0.08394788]\n",
      " [0.60879988]\n",
      " [0.10056848]\n",
      " [0.10846244]\n",
      " [0.05991844]\n",
      " [0.05851278]\n",
      " [0.04276562]\n",
      " [0.03096362]\n",
      " [0.08622478]\n",
      " [0.12228725]\n",
      " [0.32611891]\n",
      " [0.03559113]\n",
      " [0.07585292]\n",
      " [0.05426843]\n",
      " [0.18216428]\n",
      " [0.65198073]\n",
      " [0.12201412]\n",
      " [0.04672466]\n",
      " [0.12893657]\n",
      " [0.53107273]\n",
      " [0.08272786]\n",
      " [0.03076065]\n",
      " [0.06453658]\n",
      " [0.08876744]\n",
      " [0.60900292]\n",
      " [0.20339346]\n",
      " [0.43175792]\n",
      " [0.40050185]\n",
      " [0.60852048]\n",
      " [0.50135546]\n",
      " [0.61184467]\n",
      " [0.62113203]\n",
      " [0.07766516]\n",
      " [0.63133873]\n",
      " [0.07831709]\n",
      " [0.12047596]\n",
      " [0.59836998]\n",
      " [0.19785462]\n",
      " [0.10220786]\n",
      " [0.17519315]\n",
      " [0.02410341]\n",
      " [0.06990197]\n",
      " [0.12756765]\n",
      " [0.02588145]\n",
      " [0.58575009]\n",
      " [0.04852799]\n",
      " [0.19720837]\n",
      " [0.08555233]\n",
      " [0.33667148]\n",
      " [0.08353938]\n",
      " [0.05817823]\n",
      " [0.07838938]\n",
      " [0.08570896]\n",
      " [0.1137234 ]\n",
      " [0.19540591]\n",
      " [0.22216765]\n",
      " [0.56860302]\n",
      " [0.04043861]\n",
      " [0.05640536]\n",
      " [0.06850722]\n",
      " [0.22997307]\n",
      " [0.10897494]\n",
      " [0.64363496]\n",
      " [0.26864468]\n",
      " [0.33289335]\n",
      " [0.58981072]\n",
      " [0.64704868]\n",
      " [0.55429123]\n",
      " [0.4679039 ]\n",
      " [0.26560232]\n",
      " [0.58078167]\n",
      " [0.05922776]\n",
      " [0.33378327]\n",
      " [0.12409117]\n",
      " [0.72756687]\n",
      " [0.56165501]\n",
      " [0.04964956]\n",
      " [0.05190321]\n",
      " [0.5344913 ]\n",
      " [0.4375709 ]\n",
      " [0.02824342]\n",
      " [0.0099196 ]\n",
      " [0.06606006]\n",
      " [0.35901188]\n",
      " [0.08314291]\n",
      " [0.17316579]\n",
      " [0.20514648]\n",
      " [0.08821216]\n",
      " [0.02299433]\n",
      " [0.6472614 ]\n",
      " [0.03673489]\n",
      " [0.66964806]\n",
      " [0.02239655]\n",
      " [0.62326698]\n",
      " [0.05192374]\n",
      " [0.09762151]\n",
      " [0.09719799]\n",
      " [0.09094141]\n",
      " [0.01858938]\n",
      " [0.07423411]\n",
      " [0.04788681]\n",
      " [0.3803874 ]\n",
      " [0.09776245]\n",
      " [0.07936493]\n",
      " [0.07586916]\n",
      " [0.64712522]\n",
      " [0.71756893]\n",
      " [0.12532842]\n",
      " [0.0350429 ]\n",
      " [0.04877752]\n",
      " [0.43847209]\n",
      " [0.22581865]\n",
      " [0.09706758]\n",
      " [0.06543163]\n",
      " [0.10663977]\n",
      " [0.29603711]\n",
      " [0.12235098]\n",
      " [0.05172285]\n",
      " [0.0579504 ]\n",
      " [0.46385695]\n",
      " [0.15557295]\n",
      " [0.13184094]\n",
      " [0.12325832]\n",
      " [0.06686834]\n",
      " [0.13657315]\n",
      " [0.05938055]\n",
      " [0.05177805]\n",
      " [0.43322194]\n",
      " [0.19199224]\n",
      " [0.19128849]\n",
      " [0.08687823]\n",
      " [0.1113452 ]\n",
      " [0.5136745 ]\n",
      " [0.59102094]\n",
      " [0.25838308]\n",
      " [0.07036823]\n",
      " [0.0466385 ]\n",
      " [0.63170939]\n",
      " [0.40670471]\n",
      " [0.08244818]\n",
      " [0.18327841]\n",
      " [0.09006206]\n",
      " [0.14005172]\n",
      " [0.06253812]\n",
      " [0.11721414]\n",
      " [0.07280554]\n",
      " [0.08577565]\n",
      " [0.06568923]\n",
      " [0.05598926]\n",
      " [0.08807407]\n",
      " [0.11345798]\n",
      " [0.69226807]\n",
      " [0.10178455]\n",
      " [0.57100272]\n",
      " [0.33279985]\n",
      " [0.04502375]\n",
      " [0.04685282]\n",
      " [0.04808212]\n",
      " [0.1388817 ]\n",
      " [0.17749907]\n",
      " [0.05637104]\n",
      " [0.06996818]\n",
      " [0.71700772]\n",
      " [0.43274372]\n",
      " [0.18165632]\n",
      " [0.01858155]\n",
      " [0.61099432]\n",
      " [0.03869846]\n",
      " [0.30257997]\n",
      " [0.62142143]\n",
      " [0.45556731]\n",
      " [0.44454387]\n",
      " [0.10053951]\n",
      " [0.57116294]\n",
      " [0.03853015]\n",
      " [0.05797439]\n",
      " [0.08441429]\n",
      " [0.60313692]\n",
      " [0.14167126]\n",
      " [0.53141687]\n",
      " [0.11566785]\n",
      " [0.03568826]\n",
      " [0.43095112]\n",
      " [0.58339762]\n",
      " [0.3950504 ]\n",
      " [0.56414606]\n",
      " [0.27408225]\n",
      " [0.72037933]\n",
      " [0.06019767]\n",
      " [0.03963785]\n",
      " [0.23385093]\n",
      " [0.1144857 ]\n",
      " [0.12032861]\n",
      " [0.03269965]\n",
      " [0.07313987]\n",
      " [0.16712096]\n",
      " [0.06727757]\n",
      " [0.06146942]\n",
      " [0.26801706]\n",
      " [0.06125961]\n",
      " [0.02583744]\n",
      " [0.01812981]\n",
      " [0.05500833]\n",
      " [0.52562983]\n",
      " [0.1168834 ]\n",
      " [0.05402408]\n",
      " [0.51859594]\n",
      " [0.06214232]\n",
      " [0.11132929]\n",
      " [0.02778503]\n",
      " [0.03800128]\n",
      " [0.0528066 ]\n",
      " [0.02047561]\n",
      " [0.7824078 ]\n",
      " [0.45884744]\n",
      " [0.13088334]\n",
      " [0.09914817]\n",
      " [0.0321947 ]\n",
      " [0.05745054]\n",
      " [0.26364904]\n",
      " [0.1334949 ]\n",
      " [0.43813826]\n",
      " [0.50375921]\n",
      " [0.06093792]\n",
      " [0.62628999]\n",
      " [0.03819715]\n",
      " [0.23488593]\n",
      " [0.05348663]\n",
      " [0.41091911]\n",
      " [0.55777247]\n",
      " [0.46414584]\n",
      " [0.07758274]\n",
      " [0.03781511]\n",
      " [0.05422935]\n",
      " [0.57183795]\n",
      " [0.15557508]\n",
      " [0.6207954 ]\n",
      " [0.07461194]\n",
      " [0.10329986]\n",
      " [0.08578126]\n",
      " [0.50536162]\n",
      " [0.61955675]\n",
      " [0.02971676]\n",
      " [0.10676293]\n",
      " [0.08377544]\n",
      " [0.06773033]\n",
      " [0.27703645]\n",
      " [0.25516086]\n",
      " [0.04932611]\n",
      " [0.72041536]\n",
      " [0.60915143]\n",
      " [0.67848432]\n",
      " [0.10366807]\n",
      " [0.04335439]\n",
      " [0.23216947]\n",
      " [0.05566701]\n",
      " [0.23210129]\n",
      " [0.06795925]\n",
      " [0.57943942]\n",
      " [0.03416603]\n",
      " [0.02520339]\n",
      " [0.57562398]\n",
      " [0.05092244]\n",
      " [0.61266431]\n",
      " [0.05999785]\n",
      " [0.18517835]\n",
      " [0.08586652]\n",
      " [0.60971192]\n",
      " [0.22533122]\n",
      " [0.57026003]\n",
      " [0.0262194 ]\n",
      " [0.17109701]\n",
      " [0.091466  ]\n",
      " [0.02830273]\n",
      " [0.10679756]\n",
      " [0.60111696]\n",
      " [0.59214991]\n",
      " [0.50591799]\n",
      " [0.34366365]\n",
      " [0.56312864]\n",
      " [0.04465147]\n",
      " [0.00893011]\n",
      " [0.61366533]\n",
      " [0.43222598]\n",
      " [0.03000654]\n",
      " [0.10284154]\n",
      " [0.16808228]\n",
      " [0.12533359]\n",
      " [0.15929009]\n",
      " [0.63673987]\n",
      " [0.08909754]\n",
      " [0.0492947 ]\n",
      " [0.07156484]\n",
      " [0.42057479]\n",
      " [0.12415429]\n",
      " [0.10289804]\n",
      " [0.55753588]\n",
      " [0.0383006 ]\n",
      " [0.06070977]\n",
      " [0.09053539]\n",
      " [0.70096953]\n",
      " [0.5419597 ]\n",
      " [0.26678698]\n",
      " [0.23582135]\n",
      " [0.54214662]\n",
      " [0.07759491]\n",
      " [0.07864631]\n",
      " [0.07942346]\n",
      " [0.05823663]\n",
      " [0.28443888]\n",
      " [0.21883605]\n",
      " [0.09561198]\n",
      " [0.05363976]\n",
      " [0.09381011]\n",
      " [0.04042275]\n",
      " [0.64325226]\n",
      " [0.30255598]\n",
      " [0.29540574]\n",
      " [0.58475973]\n",
      " [0.15756115]\n",
      " [0.04927343]\n",
      " [0.04314914]\n",
      " [0.57219309]\n",
      " [0.12146012]\n",
      " [0.02734203]\n",
      " [0.0826475 ]\n",
      " [0.09912054]\n",
      " [0.05232592]\n",
      " [0.34055086]\n",
      " [0.07857922]\n",
      " [0.03644511]\n",
      " [0.04677058]\n",
      " [0.0755024 ]\n",
      " [0.12597297]\n",
      " [0.08687603]\n",
      " [0.35052048]\n",
      " [0.04404829]\n",
      " [0.07954276]\n",
      " [0.70334596]\n",
      " [0.06114535]\n",
      " [0.05655473]\n",
      " [0.102116  ]\n",
      " [0.05677694]\n",
      " [0.52875811]\n",
      " [0.09562918]\n",
      " [0.1050528 ]\n",
      " [0.19606849]\n",
      " [0.06190926]\n",
      " [0.33094174]\n",
      " [0.06361075]\n",
      " [0.53526204]\n",
      " [0.0282044 ]\n",
      " [0.12186267]\n",
      " [0.08123323]\n",
      " [0.10352382]\n",
      " [0.06539283]\n",
      " [0.22775961]\n",
      " [0.20667789]\n",
      " [0.07687929]\n",
      " [0.34686161]\n",
      " [0.21426559]\n",
      " [0.05453885]\n",
      " [0.1411401 ]\n",
      " [0.10955549]\n",
      " [0.04220926]\n",
      " [0.05539275]\n",
      " [0.08146093]\n",
      " [0.21287814]\n",
      " [0.24313262]\n",
      " [0.13230608]\n",
      " [0.09144216]\n",
      " [0.69484604]\n",
      " [0.04466547]\n",
      " [0.62045219]\n",
      " [0.05052986]\n",
      " [0.03612785]\n",
      " [0.22440269]\n",
      " [0.0364195 ]\n",
      " [0.35646167]\n",
      " [0.09732845]\n",
      " [0.03517528]\n",
      " [0.06853653]\n",
      " [0.04170104]\n",
      " [0.02647209]\n",
      " [0.49163152]\n",
      " [0.07410947]\n",
      " [0.53042627]\n",
      " [0.06472577]\n",
      " [0.16315549]\n",
      " [0.11026243]\n",
      " [0.10792813]\n",
      " [0.57901018]\n",
      " [0.06880913]\n",
      " [0.62999659]\n",
      " [0.13490907]\n",
      " [0.03576461]\n",
      " [0.18826482]\n",
      " [0.04710332]\n",
      " [0.03868332]\n",
      " [0.07136745]\n",
      " [0.43326682]\n",
      " [0.69873561]\n",
      " [0.64701057]\n",
      " [0.44211486]\n",
      " [0.10042397]\n",
      " [0.0595622 ]\n",
      " [0.60584071]\n",
      " [0.07565062]\n",
      " [0.08279574]\n",
      " [0.65350316]\n",
      " [0.07137616]\n",
      " [0.43485381]\n",
      " [0.5737811 ]\n",
      " [0.1146766 ]\n",
      " [0.07922174]\n",
      " [0.08771912]\n",
      " [0.06504829]\n",
      " [0.06682588]\n",
      " [0.06036259]\n",
      " [0.0966984 ]\n",
      " [0.45186751]\n",
      " [0.0874323 ]\n",
      " [0.45722101]\n",
      " [0.41178368]\n",
      " [0.1220757 ]\n",
      " [0.27878181]\n",
      " [0.08125304]\n",
      " [0.08874199]\n",
      " [0.28668973]\n",
      " [0.18301745]\n",
      " [0.24821857]\n",
      " [0.24752959]\n",
      " [0.52113827]\n",
      " [0.07016818]\n",
      " [0.09068055]\n",
      " [0.06075393]\n",
      " [0.12341199]\n",
      " [0.49407212]\n",
      " [0.05924262]\n",
      " [0.03137916]\n",
      " [0.52308342]\n",
      " [0.03596935]\n",
      " [0.04250442]\n",
      " [0.07148955]\n",
      " [0.12031865]\n",
      " [0.5791756 ]\n",
      " [0.11989604]\n",
      " [0.05882545]\n",
      " [0.53006926]\n",
      " [0.04421677]\n",
      " [0.05584119]\n",
      " [0.06830155]\n",
      " [0.57868498]\n",
      " [0.70538531]\n",
      " [0.19301094]\n",
      " [0.06957238]\n",
      " [0.34831963]\n",
      " [0.66137263]\n",
      " [0.08460575]\n",
      " [0.13608686]\n",
      " [0.08532417]\n",
      " [0.17488919]\n",
      " [0.12850731]\n",
      " [0.03687296]\n",
      " [0.60469018]\n",
      " [0.59504809]\n",
      " [0.30256406]\n",
      " [0.20576395]\n",
      " [0.05739109]\n",
      " [0.49603874]\n",
      " [0.60202651]\n",
      " [0.07686707]\n",
      " [0.06714975]\n",
      " [0.46683315]\n",
      " [0.35356649]\n",
      " [0.10213792]\n",
      " [0.43716877]\n",
      " [0.10011204]\n",
      " [0.07002502]\n",
      " [0.33075649]\n",
      " [0.08937627]\n",
      " [0.26007942]\n",
      " [0.08133432]\n",
      " [0.01563168]\n",
      " [0.35883227]\n",
      " [0.55667564]\n",
      " [0.6161593 ]\n",
      " [0.57290421]\n",
      " [0.62742616]\n",
      " [0.22721111]\n",
      " [0.40619011]\n",
      " [0.58603948]\n",
      " [0.03788329]\n",
      " [0.05004418]\n",
      " [0.43525142]\n",
      " [0.19354271]\n",
      " [0.06273991]\n",
      " [0.10562026]\n",
      " [0.11541931]\n",
      " [0.03500984]\n",
      " [0.117872  ]\n",
      " [0.02414158]\n",
      " [0.66167355]\n",
      " [0.50807055]\n",
      " [0.06297614]\n",
      " [0.55986246]\n",
      " [0.61522351]\n",
      " [0.10186153]\n",
      " [0.19743144]\n",
      " [0.49426994]\n",
      " [0.06668765]\n",
      " [0.26864626]\n",
      " [0.04814897]\n",
      " [0.0815363 ]\n",
      " [0.03923148]\n",
      " [0.4816798 ]\n",
      " [0.15636797]\n",
      " [0.06088578]\n",
      " [0.54778328]\n",
      " [0.07794086]\n",
      " [0.02688173]\n",
      " [0.67368897]\n",
      " [0.09760755]\n",
      " [0.1450412 ]\n",
      " [0.4583746 ]\n",
      " [0.4030673 ]\n",
      " [0.03330405]\n",
      " [0.0747232 ]\n",
      " [0.51044113]\n",
      " [0.04510848]\n",
      " [0.16552219]\n",
      " [0.04703227]\n",
      " [0.03371546]\n",
      " [0.1498884 ]\n",
      " [0.21360444]\n",
      " [0.09502429]\n",
      " [0.03819489]\n",
      " [0.59566714]\n",
      " [0.03819982]\n",
      " [0.05122462]\n",
      " [0.31587927]\n",
      " [0.03748272]\n",
      " [0.01223922]\n",
      " [0.18130847]\n",
      " [0.17187814]\n",
      " [0.04688534]\n",
      " [0.59158805]\n",
      " [0.54768533]\n",
      " [0.1177479 ]\n",
      " [0.56959895]\n",
      " [0.62807233]\n",
      " [0.59371129]\n",
      " [0.04453444]\n",
      " [0.27008254]\n",
      " [0.66859848]\n",
      " [0.65451284]\n",
      " [0.6128428 ]\n",
      " [0.05818596]\n",
      " [0.10379147]\n",
      " [0.12065374]\n",
      " [0.06479707]\n",
      " [0.29116725]\n",
      " [0.13424389]\n",
      " [0.16347825]\n",
      " [0.7290078 ]\n",
      " [0.04421198]\n",
      " [0.45921992]\n",
      " [0.08538306]\n",
      " [0.70488775]\n",
      " [0.5145292 ]\n",
      " [0.1083373 ]\n",
      " [0.61335807]\n",
      " [0.04677067]\n",
      " [0.17397463]\n",
      " [0.11091729]\n",
      " [0.05086162]\n",
      " [0.19588113]\n",
      " [0.48341032]\n",
      " [0.06448616]\n",
      " [0.09152796]\n",
      " [0.46821491]\n",
      " [0.12843143]\n",
      " [0.17934234]\n",
      " [0.05127   ]\n",
      " [0.05277434]\n",
      " [0.07343513]\n",
      " [0.07506762]\n",
      " [0.69028162]\n",
      " [0.23914243]\n",
      " [0.07833866]\n",
      " [0.04144786]\n",
      " [0.44821643]\n",
      " [0.09991461]\n",
      " [0.03004795]\n",
      " [0.25771831]\n",
      " [0.07690234]\n",
      " [0.0338699 ]\n",
      " [0.01541892]\n",
      " [0.04559644]\n",
      " [0.03631235]\n",
      " [0.65236343]\n",
      " [0.0561208 ]\n",
      " [0.07920621]\n",
      " [0.07355627]\n",
      " [0.00642889]\n",
      " [0.01687559]\n",
      " [0.08059356]\n",
      " [0.54923616]\n",
      " [0.14991948]\n",
      " [0.06516432]\n",
      " [0.10314838]\n",
      " [0.12943332]\n",
      " [0.04573114]\n",
      " [0.10074583]\n",
      " [0.04109198]\n",
      " [0.05956703]\n",
      " [0.20346698]\n",
      " [0.02392816]\n",
      " [0.03656011]\n",
      " [0.07522298]\n",
      " [0.50340202]\n",
      " [0.12068023]\n",
      " [0.06280236]\n",
      " [0.25824355]\n",
      " [0.11797044]\n",
      " [0.42540825]\n",
      " [0.50613263]\n",
      " [0.0702956 ]\n",
      " [0.68736955]\n",
      " [0.62008632]\n",
      " [0.07693456]\n",
      " [0.10726   ]\n",
      " [0.36413188]\n",
      " [0.04919199]\n",
      " [0.05948093]\n",
      " [0.0645371 ]\n",
      " [0.27302378]\n",
      " [0.08951973]\n",
      " [0.09752266]\n",
      " [0.05786115]\n",
      " [0.03402653]\n",
      " [0.04281733]\n",
      " [0.17667581]\n",
      " [0.2988826 ]\n",
      " [0.58542048]\n",
      " [0.08163025]\n",
      " [0.12921491]\n",
      " [0.05407704]\n",
      " [0.13762085]\n",
      " [0.40714697]\n",
      " [0.07964987]\n",
      " [0.08372035]\n",
      " [0.04512044]\n",
      " [0.06370982]\n",
      " [0.06786951]\n",
      " [0.77405308]\n",
      " [0.08813199]\n",
      " [0.02472429]\n",
      " [0.04936004]\n",
      " [0.02503842]\n",
      " [0.08046788]\n",
      " [0.04602903]\n",
      " [0.23692457]\n",
      " [0.07281761]\n",
      " [0.07205875]\n",
      " [0.13199113]\n",
      " [0.25076911]\n",
      " [0.05107756]\n",
      " [0.32938831]\n",
      " [0.65699684]\n",
      " [0.0310385 ]\n",
      " [0.60430271]\n",
      " [0.27036942]\n",
      " [0.62777647]\n",
      " [0.17490772]\n",
      " [0.14369802]\n",
      " [0.10561024]\n",
      " [0.58683013]\n",
      " [0.0448134 ]\n",
      " [0.31350407]\n",
      " [0.1235945 ]\n",
      " [0.02007653]\n",
      " [0.56952117]\n",
      " [0.02483241]\n",
      " [0.0913458 ]\n",
      " [0.58816099]\n",
      " [0.08807164]\n",
      " [0.02733562]\n",
      " [0.07211114]\n",
      " [0.58063861]\n",
      " [0.1211913 ]\n",
      " [0.31381738]\n",
      " [0.43437918]\n",
      " [0.05777543]\n",
      " [0.04447913]\n",
      " [0.09180915]\n",
      " [0.22419093]\n",
      " [0.02718575]\n",
      " [0.13274259]\n",
      " [0.10162362]\n",
      " [0.01116359]\n",
      " [0.21997228]\n",
      " [0.54523982]\n",
      " [0.06570567]\n",
      " [0.25412206]\n",
      " [0.05087693]\n",
      " [0.22009856]\n",
      " [0.09230735]\n",
      " [0.69940653]\n",
      " [0.2557779 ]\n",
      " [0.13285226]\n",
      " [0.55663804]\n",
      " [0.43638598]\n",
      " [0.08330562]\n",
      " [0.70040352]\n",
      " [0.23739632]\n",
      " [0.87098455]\n",
      " [0.69576255]\n",
      " [0.52653539]\n",
      " [0.0772635 ]\n",
      " [0.45395513]\n",
      " [0.59999573]\n",
      " [0.34111473]\n",
      " [0.27338117]\n",
      " [0.44571306]\n",
      " [0.68963539]\n",
      " [0.06879547]\n",
      " [0.08591229]\n",
      " [0.26388168]\n",
      " [0.1247337 ]\n",
      " [0.10762892]\n",
      " [0.58403879]\n",
      " [0.15590624]\n",
      " [0.09232318]\n",
      " [0.04532515]\n",
      " [0.53756188]\n",
      " [0.13356141]\n",
      " [0.08685226]\n",
      " [0.08298887]\n",
      " [0.54579929]\n",
      " [0.10094454]\n",
      " [0.06583979]\n",
      " [0.23053101]\n",
      " [0.31565695]\n",
      " [0.3921956 ]\n",
      " [0.03669022]\n",
      " [0.58754727]\n",
      " [0.05125169]\n",
      " [0.22434193]\n",
      " [0.3272513 ]\n",
      " [0.0665014 ]\n",
      " [0.6214829 ]\n",
      " [0.09037094]\n",
      " [0.57179444]\n",
      " [0.07149305]\n",
      " [0.08296508]\n",
      " [0.05656659]\n",
      " [0.57585686]\n",
      " [0.03800731]\n",
      " [0.04978857]\n",
      " [0.12953781]\n",
      " [0.04183349]\n",
      " [0.21320068]\n",
      " [0.03143061]\n",
      " [0.17358876]\n",
      " [0.0735232 ]\n",
      " [0.54397414]\n",
      " [0.11841057]\n",
      " [0.06710793]\n",
      " [0.0789326 ]\n",
      " [0.63368564]\n",
      " [0.1705995 ]\n",
      " [0.09491317]\n",
      " [0.13666563]\n",
      " [0.30040931]\n",
      " [0.11469874]\n",
      " [0.5289191 ]\n",
      " [0.05396039]\n",
      " [0.39334416]\n",
      " [0.06565311]\n",
      " [0.5649197 ]\n",
      " [0.33034847]\n",
      " [0.05432393]\n",
      " [0.61146854]\n",
      " [0.03750344]\n",
      " [0.12398295]\n",
      " [0.48450959]\n",
      " [0.02860228]\n",
      " [0.40261999]\n",
      " [0.59598409]\n",
      " [0.04722314]\n",
      " [0.07879793]\n",
      " [0.03416725]\n",
      " [0.14253142]\n",
      " [0.17862847]\n",
      " [0.60568528]\n",
      " [0.53267247]\n",
      " [0.105682  ]\n",
      " [0.18453275]\n",
      " [0.01859893]\n",
      " [0.03636575]\n",
      " [0.08484869]\n",
      " [0.06126931]\n",
      " [0.05185192]\n",
      " [0.13568843]\n",
      " [0.69597406]\n",
      " [0.62249105]\n",
      " [0.01776939]\n",
      " [0.31042932]\n",
      " [0.08136633]\n",
      " [0.06289838]\n",
      " [0.12050261]\n",
      " [0.15921646]\n",
      " [0.07947869]\n",
      " [0.11621976]\n",
      " [0.69841107]\n",
      " [0.53149664]\n",
      " [0.07933985]\n",
      " [0.16075584]\n",
      " [0.07666092]\n",
      " [0.07201415]\n",
      " [0.25157571]\n",
      " [0.08358544]\n",
      " [0.10168065]\n",
      " [0.08056436]\n",
      " [0.06157006]\n",
      " [0.09462783]\n",
      " [0.25996738]\n",
      " [0.58157831]\n",
      " [0.06117065]\n",
      " [0.41687078]\n",
      " [0.16331399]\n",
      " [0.43161526]\n",
      " [0.02712119]\n",
      " [0.04121626]\n",
      " [0.21328676]\n",
      " [0.10235003]\n",
      " [0.10143744]\n",
      " [0.11900188]\n",
      " [0.17101483]\n",
      " [0.11110445]\n",
      " [0.09139647]\n",
      " [0.26973772]\n",
      " [0.34308456]\n",
      " [0.04852332]\n",
      " [0.5413746 ]\n",
      " [0.03930415]\n",
      " [0.60628409]\n",
      " [0.08035927]\n",
      " [0.03659993]\n",
      " [0.56730178]\n",
      " [0.57771881]\n",
      " [0.07778053]\n",
      " [0.32910362]\n",
      " [0.34810486]\n",
      " [0.54550232]\n",
      " [0.37067634]\n",
      " [0.12184255]\n",
      " [0.56849916]\n",
      " [0.11934725]\n",
      " [0.59999063]\n",
      " [0.54942932]\n",
      " [0.06956862]\n",
      " [0.36936889]\n",
      " [0.07759169]\n",
      " [0.02435694]\n",
      " [0.0567111 ]\n",
      " [0.62746471]\n",
      " [0.17471814]\n",
      " [0.31942753]\n",
      " [0.05952845]\n",
      " [0.09778395]\n",
      " [0.03376521]\n",
      " [0.39340979]\n",
      " [0.41509635]\n",
      " [0.14477422]\n",
      " [0.06438466]\n",
      " [0.05161425]\n",
      " [0.07996778]\n",
      " [0.01089328]\n",
      " [0.08895003]\n",
      " [0.11994509]\n",
      " [0.49038687]\n",
      " [0.09614811]\n",
      " [0.08088854]\n",
      " [0.15864985]\n",
      " [0.05499478]\n",
      " [0.57115516]\n",
      " [0.46338942]\n",
      " [0.3710247 ]\n",
      " [0.4940957 ]\n",
      " [0.09361797]\n",
      " [0.42655001]\n",
      " [0.04754498]\n",
      " [0.37326176]\n",
      " [0.07376422]\n",
      " [0.51949111]\n",
      " [0.07853468]\n",
      " [0.05285566]\n",
      " [0.14779065]\n",
      " [0.48897631]\n",
      " [0.15911308]\n",
      " [0.07976795]\n",
      " [0.21288007]\n",
      " [0.43987944]\n",
      " [0.13489854]\n",
      " [0.63612836]\n",
      " [0.38963651]\n",
      " [0.5989446 ]\n",
      " [0.1179644 ]\n",
      " [0.02039772]\n",
      " [0.05431259]\n",
      " [0.12042394]\n",
      " [0.22685892]\n",
      " [0.08956669]\n",
      " [0.49517758]\n",
      " [0.0115405 ]\n",
      " [0.04747891]\n",
      " [0.10808534]\n",
      " [0.14287267]\n",
      " [0.10138925]\n",
      " [0.04971123]\n",
      " [0.44410453]\n",
      " [0.04722107]\n",
      " [0.04480287]\n",
      " [0.01599195]\n",
      " [0.03629491]\n",
      " [0.63992898]\n",
      " [0.06071218]\n",
      " [0.05174795]\n",
      " [0.05244389]\n",
      " [0.15349329]\n",
      " [0.06976703]\n",
      " [0.0916401 ]\n",
      " [0.52703584]\n",
      " [0.01833537]\n",
      " [0.22833659]\n",
      " [0.06328647]\n",
      " [0.14618106]\n",
      " [0.54617287]\n",
      " [0.07267107]\n",
      " [0.59202432]\n",
      " [0.06490546]\n",
      " [0.13637292]\n",
      " [0.06634065]\n",
      " [0.05857056]\n",
      " [0.06385902]\n",
      " [0.06221703]\n",
      " [0.13107453]\n",
      " [0.59772558]\n",
      " [0.28265306]\n",
      " [0.07050849]\n",
      " [0.07217843]\n",
      " [0.09588995]\n",
      " [0.57786474]\n",
      " [0.07418049]\n",
      " [0.12215044]\n",
      " [0.12664635]\n",
      " [0.09741975]\n",
      " [0.17106699]\n",
      " [0.562778  ]\n",
      " [0.09600185]\n",
      " [0.3109394 ]\n",
      " [0.07308454]\n",
      " [0.11264314]\n",
      " [0.0810189 ]\n",
      " [0.06814912]\n",
      " [0.17914524]\n",
      " [0.09852809]\n",
      " [0.11850084]\n",
      " [0.15571753]\n",
      " [0.07473962]\n",
      " [0.02086293]\n",
      " [0.58025003]\n",
      " [0.11644769]\n",
      " [0.13333479]\n",
      " [0.05031825]\n",
      " [0.04199955]\n",
      " [0.04442665]\n",
      " [0.54441904]\n",
      " [0.43504927]\n",
      " [0.63731777]\n",
      " [0.16846526]\n",
      " [0.11245482]\n",
      " [0.81767579]\n",
      " [0.38423394]\n",
      " [0.21319513]\n",
      " [0.15668303]\n",
      " [0.03173308]\n",
      " [0.20237886]\n",
      " [0.12393041]\n",
      " [0.08536786]\n",
      " [0.03776507]\n",
      " [0.04437529]\n",
      " [0.08470208]\n",
      " [0.11737054]\n",
      " [0.47569366]\n",
      " [0.75808082]\n",
      " [0.32066019]]\n"
     ]
    }
   ],
   "source": [
    "df_xtest = decision_function(X_test)\n",
    "\n",
    "for i in X_test:\n",
    "    xx = 1/( 1 + np.exp( -1*(  (w * df_xtest)+b  )  )  )\n",
    "    \n",
    "print(\"P==1|X (for every pointin Xtest): \",xx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTY7z2bd4Zx2"
   },
   "source": [
    "__Note: in the above algorithm, the steps 2, 4 might need hyper parameter tuning, To reduce the complexity of the assignment we are excluding the hyerparameter tuning part, but intrested students can try that__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CM3odN1Z4Zx3"
   },
   "source": [
    "\n",
    "If any one wants to try other calibration algorithm istonic regression also please check these tutorials\n",
    "\n",
    "1. http://fa.bianp.net/blog/tag/scikit-learn.html#fn:1\n",
    "\n",
    "2. https://drive.google.com/open?id=1MzmA7QaP58RDzocB0RBmRiWfl7Co_VJ7\n",
    "\n",
    "3. https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a\n",
    "\n",
    "4. https://stat.fandom.com/wiki/Isotonic_regression#Pool_Adjacent_Violators_Algorithm\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "8E&F_LR_SVM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
